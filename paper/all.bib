
@article{zheng_cross-line_2019,
	title = {Cross-Line Pedestrian Counting Based on Spatially-Consistent Two-Stage Local Crowd Density Estimation and Accumulation},
	volume = {29},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2018.2807806},
	abstract = {This paper proposes a scalable approach for counting pedestrians crossing a virtual line when the crowd is highly dynamic and possibly extremely dense. The approach mainly consists of two parts: local crowd density estimation and pedestrian counting based on accumulating local densities across the line. To obtain a fine estimation of local crowd densities, we divide the neighborhood at the line into a number of blocks. We enforce spatial consistency between local counts in the blocks and those in the enclosing regions to guarantee consistent estimation of local crowd densities. For scalability to various density levels in crowd density estimation, we propose a two-stage strategy: pre-classification of density levels and subsequent regression with overlapped operational ranges. To count pedestrians crossing the virtual line, we accumulate the crowd densities across the line according to the locally estimated velocities. Extensive experimental results demonstrate the effectiveness of the proposed approach and its scalability to crowdedness.},
	pages = {787--799},
	number = {3},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Zheng, Huicheng and Lin, Zijian and Cen, Jiepeng and Wu, Zeyu and Zhao, Yadan},
	date = {2019-03},
	year={2019},
	keywords = {Feature extraction, image sequences, regression analysis, Cameras, consistent estimation, cross-line counting, cross-line pedestrian counting, crowd density estimation, density levels, Estimation, fine estimation, Head, image motion analysis, image segmentation, local counts, local crowd densities, local crowd density estimation, local densities, locally estimated velocities, object detection, Pedestrian counting, pedestrians, Reliability, Scalability, spatial consistency, spatially-consistent two-stage, Support vector machines, traffic engineering computing, virtual line},
	file = {IEEE Xplore Full Text PDF:/home/janerik/Zotero/storage/N6TEYV4M/Zheng et al. - 2019 - Cross-Line Pedestrian Counting Based on Spatially-.pdf:application/pdf;IEEE Xplore Abstract Record:/home/janerik/Zotero/storage/Y44HJMRQ/8295124.html:text/html}
}

@article{ma_counting_2016,
	title = {Counting People Crossing a Line Using Integer Programming and Local Features},
	volume = {26},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2015.2489418},
	abstract = {We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest ({LOI}) in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that the count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the {LOI} in each frame. Integrating over a specific time interval yields the cumulative count of pedestrians crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video data sets.},
	pages = {1955--1969},
	number = {10},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Ma, Zheng and Chan, Antoni B.},
	date = {2016-10},
	keywords = {Feature extraction, image sequences, Kernel, line of interest, regression analysis, Cameras, Crowd counting, Histograms, Image edge detection, Image segmentation, integer programming, integer programming method, line sampling process, Linear programming, local feature, local features, {LOI}, regression function, temporal slice image, video sequence},
	file = {IEEE Xplore Full Text PDF:/home/janerik/Zotero/storage/HS6ABMDT/Ma and Chan - 2016 - Counting People Crossing a Line Using Integer Prog.pdf:application/pdf;IEEE Xplore Abstract Record:/home/janerik/Zotero/storage/HS9AZF7L/7295569.html:text/html}
}

@article{Ma2013,
abstract = {We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets. {\textcopyright} 2013 IEEE.},
author = {Ma, Zheng and Chan, Antoni B.},
doi = {10.1109/CVPR.2013.328},
file = {:Users/janerik/Downloads/cvpr13-linecount-2.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {crowd counting,integer programming,local feature,regression},
mendeley-groups = {LOI},
pages = {2539--2546},
title = {{Crossing the line: Crowd counting by integer programming with local features}},
year = {2013}
}


@article{Dosovitskiy2015,
abstract = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
archivePrefix = {arXiv},
arxivId = {1504.06852},
author = {Dosovitskiy, Alexey and Fischery, Philipp and Ilg, Eddy and Hausser, Philip and Hazirbas, Caner and Golkov, Vladimir and Smagt, Patrick Van Der and Cremers, Daniel and Brox, Thomas},
doi = {10.1109/ICCV.2015.316},
eprint = {1504.06852},
file = {:Users/janerik/Library/Application Support/Mendeley Desktop/Downloaded/Dosovitskiy et al. - 2015 - FlowNet Learning optical flow with convolutional networks.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Flow Estimation},
pages = {2758--2766},
title = {{FlowNet: Learning optical flow with convolutional networks}},
volume = {2015 Inter},
year = {2015}
}


@article{ilg_flownet_2016,
	title = {{FlowNet} 2.0: Evolution of Optical Flow Estimation with Deep Networks},
	url = {http://arxiv.org/abs/1612.01925},
	shorttitle = {{FlowNet} 2.0},
	abstract = {The {FlowNet} demonstrated that optical ﬂow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the ﬂow has still been deﬁned by traditional methods. Particularly on small displacements and real-world data, {FlowNet} cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical ﬂow and make it work really well. The large improvements in quality and speed are caused by three major contributions: ﬁrst, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical ﬂow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. {FlowNet} 2.0 is only marginally slower than the original {FlowNet} but decreases the estimation error by more than 50\%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical ﬂow computation at up to 140fps with accuracy matching the original {FlowNet}.},
	journaltitle = {{arXiv}:1612.01925 [cs]},
	author = {Ilg, Eddy and Mayer, Nikolaus and Saikia, Tonmoy and Keuper, Margret and Dosovitskiy, Alexey and Brox, Thomas},
	urldate = {2020-02-07},
	date = {2016-12-06},
	year={2016}
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.01925},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ilg et al. - 2016 - FlowNet 2.0 Evolution of Optical Flow Estimation .pdf:/home/janerik/Zotero/storage/DM8D8K7P/Ilg et al. - 2016 - FlowNet 2.0 Evolution of Optical Flow Estimation .pdf:application/pdf}
}

@incollection{leibe_crossing-line_2016,
	location = {Cham},
	title = {Crossing-Line Crowd Counting with Two-Phase Deep Neural Networks},
	volume = {9912},
	isbn = {978-3-319-46483-1 978-3-319-46484-8},
	url = {http://link.springer.com/10.1007/978-3-319-46484-8_43},
	abstract = {In this paper, we propose a deep Convolutional Neural Network ({CNN}) for counting the number of people across a line-of-interest ({LOI}) in surveillance videos. It is a challenging problem and has many potential applications. Observing the limitations of temporal slices used by state-of-the-art {LOI} crowd counting methods, our proposed {CNN} directly estimates the crowd counts with pairs of video frames as inputs and is trained with pixel-level supervision maps. Such rich supervision information helps our {CNN} learn more discriminative feature representations. A two-phase training scheme is adopted, which decomposes the original counting problem into two easier sub-problems, estimating crowd density map and estimating crowd velocity map. Learning to solve the sub-problems provides a good initial point for our {CNN} model, which is then ﬁne-tuned to solve the original counting problem. A new dataset with pedestrian trajectory annotations is introduced for evaluating {LOI} crowd counting methods and has more annotations than any existing one. Our extensive experiments show that our proposed method is robust to variations of crowd density, crowd velocity, and directions of the {LOI}, and outperforms state-of-the-art {LOI} counting methods.},
	pages = {712--726},
	booktitle = {Computer Vision – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Zhao, Zhuoyi and Li, Hongsheng and Zhao, Rui and Wang, Xiaogang},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	urldate = {2020-02-18},
	year = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-46484-8_43},
	file = {Zhao et al. - 2016 - Crossing-Line Crowd Counting with Two-Phase Deep N.pdf:/home/janerik/Zotero/storage/VECR5FSN/Zhao et al. - 2016 - Crossing-Line Crowd Counting with Two-Phase Deep N.pdf:application/pdf}
}

@article{liu_selflow_2019,
	title = {{SelFlow}: Self-Supervised Learning of Optical Flow},
	url = {http://arxiv.org/abs/1904.09117},
	shorttitle = {{SelFlow}},
	abstract = {We present a self-supervised learning approach for optical ﬂow. Our method distills reliable ﬂow estimations from non-occluded pixels, and uses these predictions as ground truth to learn optical ﬂow for hallucinated occlusions. We further design a simple {CNN} to utilize temporal information from multiple frames for better ﬂow estimation. These two principles lead to an approach that yields the best performance for unsupervised optical ﬂow learning on the challenging benchmarks including {MPI} Sintel, {KITTI} 2012 and 2015. More notably, our self-supervised pre-trained model provides an excellent initialization for supervised ﬁne-tuning. Our ﬁne-tuned models achieve stateof-the-art results on all three datasets. At the time of writing, we achieve {EPE}=4.26 on the Sintel benchmark, outperforming all submitted methods.},
	journaltitle = {{arXiv}:1904.09117 [cs]},
	author = {Liu, Pengpeng and Lyu, Michael and King, Irwin and Xu, Jia},
	urldate = {2020-02-25},
	date = {2019-04-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.09117},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Liu et al. - 2019 - SelFlow Self-Supervised Learning of Optical Flow.pdf:/home/janerik/Zotero/storage/6RZY8NZA/Liu et al. - 2019 - SelFlow Self-Supervised Learning of Optical Flow.pdf:application/pdf}
}

@article{cao_large_2015,
	title = {Large scale crowd analysis based on convolutional neural network},
	volume = {48},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320315001259},
	doi = {10.1016/j.patcog.2015.04.001},
	abstract = {Nowadays crowd surveillance is an active area of research. Crowd surveillance is always affected by various conditions, such as different scenes, weather, or density of crowd, which restricts the real application. This paper proposes a convolutional neural network ({CNN}) based method to monitor the number of crowd ﬂow, such as the number of entering or leaving people in high density crowd. It uses an indirect strategy of combining classiﬁcation {CNN} with regression {CNN}, which is more robust than the direct way. A large enough database is built with lots of real videos of public gates, and plenty of experiments show that the proposed method performs well under various weather conditions no matter either in daytime or at night.},
	pages = {3016--3024},
	number = {10},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Cao, Lijun and Zhang, Xu and Ren, Weiqiang and Huang, Kaiqi},
	urldate = {2020-02-28},
	date = {2015-10},
	langid = {english},
	file = {Cao et al. - 2015 - Large scale crowd analysis based on convolutional .pdf:/home/janerik/Zotero/storage/Q9YWG5SH/Cao et al. - 2015 - Large scale crowd analysis based on convolutional .pdf:application/pdf}
}

@article{liu_ddflow_2019,
	title = {{DDFlow}: Learning Optical Flow with Unlabeled Data Distillation},
	url = {http://arxiv.org/abs/1902.09145},
	shorttitle = {{DDFlow}},
	abstract = {We present {DDFlow}, a data distillation approach to learning optical ﬂow estimation from unlabeled data. The approach distills reliable predictions from a teacher network, and uses these predictions as annotations to guide a student network to learn optical ﬂow. Unlike existing work relying on handcrafted energy terms to handle occlusion, our approach is data-driven, and learns optical ﬂow for occluded pixels. This enables us to train our model with a much simpler loss function, and achieve a much higher accuracy. We conduct a rigorous evaluation on the challenging Flying Chairs, {MPI} Sintel, {KITTI} 2012 and 2015 benchmarks, and show that our approach signiﬁcantly outperforms all existing unsupervised learning methods, while running at real time.},
	journaltitle = {{arXiv}:1902.09145 [cs]},
	author = {Liu, Pengpeng and King, Irwin and Lyu, Michael R. and Xu, Jia},
	urldate = {2020-03-02},
	date = {2019-02-25},
	year = {2019}
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.09145},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2019 - DDFlow Learning Optical Flow with Unlabeled Data .pdf:/home/janerik/Zotero/storage/5MMY5IVS/Liu et al. - 2019 - DDFlow Learning Optical Flow with Unlabeled Data .pdf:application/pdf}
}

@article{sun_pwc-net_2018,
	title = {{PWC}-Net: {CNNs} for Optical Flow Using Pyramid, Warping, and Cost Volume},
	url = {http://arxiv.org/abs/1709.02371},
	shorttitle = {{PWC}-Net},
	abstract = {We present a compact but effective {CNN} model for optical ﬂow, called {PWC}-Net. {PWC}-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, {PWC}-Net uses the current optical ﬂow estimate to warp the {CNN} features of the second image. It then uses the warped features and features of the ﬁrst image to construct a cost volume, which is processed by a {CNN} to estimate the optical ﬂow. {PWCNet} is 17 times smaller in size and easier to train than the recent {FlowNet}2 model. Moreover, it outperforms all published optical ﬂow methods on the {MPI} Sintel ﬁnal pass and {KITTI} 2015 benchmarks, running at about 35 fps on Sintel resolution (1024×436) images. Our models are available on https://github.com/{NVlabs}/{PWC}-Net.},
	journaltitle = {{arXiv}:1709.02371 [cs]},
	author = {Sun, Deqing and Yang, Xiaodong and Liu, Ming-Yu and Kautz, Jan},
	urldate = {2020-03-03},
	date = {2018-06-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.02371},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sun et al. - 2018 - PWC-Net CNNs for Optical Flow Using Pyramid, Warp.pdf:/home/janerik/Zotero/storage/CP3298WW/Sun et al. - 2018 - PWC-Net CNNs for Optical Flow Using Pyramid, Warp.pdf:application/pdf}
}

@manual{lucas_kan_nutshell,
    author = {Prof.  Dr.  Ra ́ul Rojas},
    title = {Lucas-Kanade in a Nutshell}
}

@article{wang2020nwpu,
  title={NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting},
  author={Wang, Qi and Gao, Junyu and Lin, Wei and Li, Xuelong},
  journal={arXiv preprint arXiv:2001.03360},
  year={2020}
}

@inproceedings{li2018csrnet,
  title={CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes},
  author={Li, Yuhong and Zhang, Xiaofan and Chen, Deming},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1091--1100},
  year={2018}
}

@article{Fang2019,
abstract = {Compared with single image based crowd counting, video provides the spatial-temporal information of the crowd that would help improve the robustness of crowd counting. But translation, rotation and scaling of people lead to the change of density map of heads between neighbouring frames. Meanwhile, people walking in/out or being occluded in dynamic scenes leads to the change of head counts. To alleviate these issues in video crowd counting, a Locality-constrained Spatial Transformer Network (LSTN) is proposed. Specifically, we first leverage a Convolutional Neural Networks to estimate the density map for each frame. Then to relate the density maps between neighbouring frames, a Locality-constrained Spatial Transformer (LST) module is introduced to estimate the density map of next frame with that of current frame. To facilitate the performance evaluation, a large-scale video crowd counting dataset is collected, which contains 15K frames with about 394K annotated heads captured from 13 different scenes. As far as we know, it is the largest video crowd counting dataset. Extensive experiments on our dataset and other crowd counting datasets validate the effectiveness of our LSTN for crowd counting. All our dataset are released in https://github.com/sweetyy83/Lstn-fdst-dataset.},
archivePrefix = {arXiv},
arxivId = {1907.07911},
author = {Fang, Yanyan and Zhan, Biyun and Cai, Wandi and Gao, Shenghua and Hu, Bo},
doi = {10.1109/ICME.2019.00145},
eprint = {1907.07911},
file = {:Users/janerik/Downloads/1907.07911.pdf:pdf},
isbn = {9781538695524},
issn = {1945788X},
journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
keywords = {Convolutional neural network,Locality constrained,Spatial transformer network,Video crowd counting},
mendeley-groups = {Datasets},
pages = {814--819},
title = {{Locality-constrained spatial transformer network for video crowd counting}},
volume = {2019-July},
year = {2019}
}

@article{Liu2019,
abstract = {State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. They typically use the same filters over the whole image or over large image patches. Only then do they estimate local scale to compensate for perspective distortion. This is typically achieved by training an auxiliary classifier to select, for predefined image patches, the best kernel size among a limited set of choices. As such, these methods are not end-to-end trainable and restricted in the scope of context they can leverage. In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. In other words, our approach adaptively encodes the scale of the contextual information required to accurately predict crowd density. This yields an algorithm that outperforms state-of-the-art crowd counting methods, especially when perspective effects are strong.},
archivePrefix = {arXiv},
arxivId = {arXiv:1811.10452v2},
author = {Liu, Weizhe and Salzmann, Mathieu and Fua, Pascal},
doi = {10.1109/CVPR.2019.00524},
eprint = {arXiv:1811.10452v2},
file = {:Users/janerik/Downloads/1811.10452-2.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval,Vision Applications and Systems,Visual Reasoning},
mendeley-groups = {Crowd Counting,To Read},
pages = {5094--5103},
title = {{Context-aware crowd counting}},
volume = {2019-June},
year = {2019}
}


@article{Schroder2019,
abstract = {The performance of optical flow algorithms greatly depends on the specifics of the content and the application for which it is used. Existing and well established optical flow datasets are limited to rather particular contents from which none is close to crowd behavior analysis; whereas such applications heavily utilize optical flow. We introduce a new optical flow dataset exploiting the possibilities of a recent video engine to generate sequences with ground-truth optical flow for large crowds in different scenarios. We break with the development of the last decade of introducing ever increasing displacements to pose new difficulties. Instead we focus on real-world surveillance scenarios where numerous small, partly independent, non rigidly moving objects observed over a long temporal range pose a challenge. By evaluating different optical flow algorithms, we find that results of established datasets can not be transferred to these new challenges. In exhaustive experiments we are able to provide new insight into optical flow for crowd analysis. Finally, the results have been validated on the real-world UCF crowd tracking benchmark while achieving competitive results compared to more sophisticated state-of-the-art crowd tracking approaches.},
annote = {Paper of the TUB dataset. So cite this boy. It is mainly done for flow estimation, but can be used for Crowd Counting as well.},
archivePrefix = {arXiv},
arxivId = {1811.07170},
author = {Schroder, Gregory and Senst, Tobias and Bochinski, Erik and Sikora, Thomas},
doi = {10.1109/AVSS.2018.8639113},
eprint = {1811.07170},
file = {:Users/janerik/Downloads/1811.07170.pdf:pdf},
isbn = {9781538692943},
journal = {Proceedings of AVSS 2018 - 2018 15th IEEE International Conference on Advanced Video and Signal-Based Surveillance},
mendeley-groups = {Crowd Counting,Datasets,To Read},
title = {{Optical Flow Dataset and Benchmark for Visual Crowd Analysis}},
year = {2019}
}

@article{Chan2008,
abstract = {A dynamic texture is a spatio-temporal generative model for video, which represents video sequences as observations from a linear dynamical system. This work studies the mixture of dynamic textures, a statistical model for an ensemble of video sequences that is sampled from a finite collection of visual processes, each of which is a dynamic texture. An expectationmaximization (EM) algorithm is derived for learning the parameters of the model, and the model is related to previous works in linear systems, machine learning, time-series clustering, control theory, and computer vision. Through experimentation, it is shown that the mixture of dynamic textures is a suitable representation for both the appearance and dynamics of a variety of visual processes that have traditionally been challenging for computer vision (e.g. fire, steam, water, vehicle and pedestrian traffic, etc.). When compared with state-of-the-art methods in motion segmentation, including both temporal texture methods and traditional representations (e.g. optical flow or other localized motion representations), the mixture of dynamic textures achieves superior performance in the problems of clustering and segmenting video of such processes. {\textcopyright} 2008 IEEE.},
author = {Chan, Antoni B. and Vasconcelos, Nuno},
doi = {10.1109/TPAMI.2007.70738},
file = {:Users/janerik/Downloads/pami08-dytexmix.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Dynamic texture,Expectation-maximization,Kalman filter,Linear dynamical systems,Mixture models,Motion segmentation,Probabilistic models,Temporal textures,Time-series clustering,Video clustering,Video modeling},
mendeley-groups = {Datasets},
number = {5},
pages = {909--926},
pmid = {18369258},
title = {{Modeling, clustering, and segmenting video with mixtures of dynamic textures}},
volume = {30},
year = {2008}
}

@article{Horn1981,
abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image. {\textcopyright} 1981.},
author = {Horn, Berthold K.P. and Schunck, Brian G.},
doi = {10.1016/0004-3702(81)90024-2},
file = {:Users/janerik/Downloads/download.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-3},
pages = {185--203},
title = {{Determining optical flow}},
volume = {17},
year = {1981}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CROWD COUNTING %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Zhang2016,
abstract = {This paper aims to develop a method than can accurately estimate the crowd count from an individual image with arbitrary crowd density and arbitrary perspective. To this end, we have proposed a simple but effective Multi-column Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map. The proposed MCNN allows the input image to be of arbitrary size or resolution. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people/head size due to perspective effect or image resolution. Furthermore, the true density map is computed accurately based on geometry-adaptive kernels which do not need knowing the perspective map of the input image. Since exiting crowd counting datasets do not adequately cover all the challenging situations considered in our work, we have collected and labelled a large new dataset that includes 1198 images with about 330,000 heads annotated. On this challenging new dataset, as well as all existing datasets, we conduct extensive experiments to verify the effectiveness of the proposed model and method. In particular, with the proposed simple MCNN model, our method outperforms all existing methods. In addition, experiments show that our model, once trained on one dataset, can be readily transferred to a new dataset.},
author = {Zhang, Yingying and Zhou, Desen and Chen, Siqin and Gao, Shenghua and Ma, Yi},
doi = {10.1109/CVPR.2016.70},
file = {:Users/janerik/Downloads/07780439.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Crowd Counting},
pages = {589--597},
title = {{Single-image crowd counting via multi-column convolutional neural network}},
volume = {2016-Decem},
year = {2016}
}


@article{Wu2007,
abstract = {Detection and tracking of humans in video streams is important for many applications. We present an approach to automatically detect and track multiple, possibly partially occluded humans in a walking or standing pose from a single camera, which may be stationary or moving. A human body is represented as an assembly of body parts. Part detectors are learned by boosting a number of weak classifiers which are based on edgelet features. Responses of part detectors are combined to form a joint likelihood model that includes an analysis of possible occlusions. The combined detection responses and the part detection responses provide the observations used for tracking. Trajectory initialization and termination are both automatic and rely on the confidences computed from the detection responses. An object is tracked by data association and meanshift methods. Our system can track humans with both inter-object and scene occlusions with static or non-static backgrounds. Evaluation results on a number of images and videos and comparisons with some previous methods are given. {\textcopyright} 2007 Springer Science+Business Media, LLC.},
author = {Wu, Bo and Nevatia, Ram},
doi = {10.1007/s11263-006-0027-7},
file = {:Users/janerik/Downloads/Wu-Nevatia2007{\_}Article{\_}DetectionAndTrackingOfMultiple.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {AdaBoost,Human detection,Human tracking},
mendeley-groups = {Crowd Counting},
number = {2},
pages = {247--266},
title = {{Detection and tracking of multiple, partially occluded humans by Bayesian combination of edgelet based part detectors}},
volume = {75},
year = {2007}
}


@article{Subburaman2012,
abstract = {Crowd counting and density estimation is still one of the important task in video surveillance. Usually a regression based method is used to estimate the number of people from a sequence of images. In this paper we investigate to estimate the count of people in a crowded scene. We detect the head region since this is the most visible part of the body in a crowded scene. The head detector is based on state-of-art cascade of boosted integral features. To prune the search region we propose a novel interest point detector based on gradient orientation feature to locate regions similar to the top of head region from gray level images. Two different background subtraction methods are evaluated to further reduce the search region. We evaluate our approach on PETS 2012 and Turin metro station databases. Experiments on these databases show good performance of our method for crowd counting. {\textcopyright} 2012 IEEE.},
author = {Subburaman, Venkatesh Bala and Descamps, Adrien and Carincotte, Cyril},
doi = {10.1109/AVSS.2012.87},
file = {:Users/janerik/Downloads/06328059.pdf:pdf},
isbn = {9780769547978},
journal = {Proceedings - 2012 IEEE 9th International Conference on Advanced Video and Signal-Based Surveillance, AVSS 2012},
mendeley-groups = {Crowd Counting},
pages = {470--475},
publisher = {IEEE},
title = {{Counting people in the crowd using a generic head detector}},
year = {2012}
}

@article{Lin2010,
abstract = {We propose a shape-based, hierarchical part-template matching approach to simultaneous human detection and segmentation combining local part-based and global shape-template-based schemes. The approach relies on the key idea of matching a part-template tree to images hierarchically to detect humans and estimate their poses. For learning a generic human detector, a pose-adaptive feature computation scheme is developed based on a tree matching approach. Instead of traditional concatenation-style image location-based feature encoding, we extract features adaptively in the context of human poses and train a kernel-SVM classifier to separate human/nonhuman patterns. Specifically, the features are collected in the local context of poses by tracing around the estimated shape boundaries. We also introduce an approach to multiple occluded human detection and segmentation based on an iterative occlusion compensation scheme. The output of our learned generic human detector can be used as an initial set of human hypotheses for the iterative optimization. We evaluate our approaches on three public pedestrian data sets (INRIA, MIT-CBCL, and USC-B) and two crowded sequences from Caviar Benchmark and Munich Airport data sets. {\textcopyright} 2010 IEEE.},
author = {Lin, Zhe and Davis, Larry S.},
doi = {10.1109/TPAMI.2009.204},
file = {:Users/janerik/Downloads/05374413.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Generic human detector,Hierarchical part-template matching,Occlusion analysis,Part-template tree,Pose-adaptive descriptor},
mendeley-groups = {Crowd Counting},
number = {4},
pages = {604--618},
pmid = {20224118},
publisher = {IEEE},
title = {{Shape-based human detection and segmentation via hierarchical part-template matching}},
volume = {32},
year = {2010}
}


@article{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. {\textcopyright} 2005 IEEE.},
author = {Dalal, Navneet and Triggs, Bill},
doi = {10.1109/CVPR.2005.177},
file = {:Users/janerik/Downloads/01467360-2.pdf:pdf},
isbn = {0769523722},
journal = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
mendeley-groups = {Crowd Counting},
pages = {886--893},
title = {{Histograms of oriented gradients for human detection}},
volume = {I},
year = {2005}
}

@article{Dollar2012,
abstract = {Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate the performance of sixteen pretrained state-of-the-art detectors across six data sets. Our study allows us to assess the state of the art and provides a framework for gauging future efforts. Our experiments show that despite significant progress, performance still has much room for improvement. In particular, detection is disappointing at low resolutions and for partially occluded pedestrians. {\textcopyright} 2012 IEEE.},
author = {Doll{\'{a}}r, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro},
doi = {10.1109/TPAMI.2011.155},
file = {:Users/janerik/Downloads/05975165.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Caltech Pedestrian data set,Pedestrian detection,benchmark,data set,evaluation,object detection},
mendeley-groups = {Crowd Counting},
number = {4},
pages = {743--761},
pmid = {21808091},
title = {{Pedestrian detection: An evaluation of the state of the art}},
volume = {34},
year = {2012}
}


@article{Chan2009,
abstract = {Poisson regression models the noisy output of a counting function as a Poisson random variable, with a log-mean parameter that is a linear function of the input vector. In this work, we analyze Poisson regression in a Bayesian setting, by introducing a prior distribution on the weights of the linear function. Since exact inference is analytically un- obtainable, we derive a closed-form approximation to the predictive distribution of the model. We show that the predictive distribution can be kernelized, enabling the representation of non-linear log-mean functions. We also derive an approximate marginal likelihood that can be optimized to learn the hyperparameters of the kernel. We then relate the proposed approximate Bayesian Poisson regression to Gaussian processes. Finally, we present experimental results using Bayesian Poisson regression for crowd counting from low-level features.},
author = {Chan, AB and Vasconcelos, N},
file = {:Users/janerik/Downloads/iccv09-bpr.pdf:pdf},
journal = {Computer Vision, 2009 IEEE 12th International  {\ldots}},
mendeley-groups = {Crowd Counting},
title = {{Bayesian Poisson regression for crowd counting Cited by me}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26start{\%}3D70{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\&}citilm=1{\&}citation{\_}for{\_}view=1c9oQNMAAAAJ:RGFaLdJalmkC{\&}hl=en{\&}oi=p},
year = {2009}
}


@article{Idrees2013,
abstract = {We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark contrast to datasets used for existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance. {\textcopyright} 2013 IEEE.},
author = {Idrees, Haroon and Saleemi, Imran and Seibert, Cody and Shah, Mubarak},
doi = {10.1109/CVPR.2013.329},
file = {:Users/janerik/Downloads/06619173.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Counting,Dense Crowds,Markov Random Field,Multi-scale Analysis},
mendeley-groups = {Crowd Counting},
pages = {2547--2554},
publisher = {IEEE},
title = {{Multi-source multi-scale counting in extremely dense crowd images}},
year = {2013}
}


@article{Zhang2016,
abstract = {This paper aims to develop a method than can accurately estimate the crowd count from an individual image with arbitrary crowd density and arbitrary perspective. To this end, we have proposed a simple but effective Multi-column Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map. The proposed MCNN allows the input image to be of arbitrary size or resolution. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people/head size due to perspective effect or image resolution. Furthermore, the true density map is computed accurately based on geometry-adaptive kernels which do not need knowing the perspective map of the input image. Since exiting crowd counting datasets do not adequately cover all the challenging situations considered in our work, we have collected and labelled a large new dataset that includes 1198 images with about 330,000 heads annotated. On this challenging new dataset, as well as all existing datasets, we conduct extensive experiments to verify the effectiveness of the proposed model and method. In particular, with the proposed simple MCNN model, our method outperforms all existing methods. In addition, experiments show that our model, once trained on one dataset, can be readily transferred to a new dataset.},
author = {Zhang, Yingying and Zhou, Desen and Chen, Siqin and Gao, Shenghua and Ma, Yi},
doi = {10.1109/CVPR.2016.70},
file = {:Users/janerik/Downloads/07780439.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Crowd Counting},
pages = {589--597},
title = {{Single-image crowd counting via multi-column convolutional neural network}},
volume = {2016-December},
year = {2016}
}


@article{Wan2019,
abstract = {Crowd counting is an important topic in computer vision due to its practical usage in surveillance systems. The typical design of crowd counting algorithms is divided into two steps. First, the ground-truth density maps of crowd images are generated from the ground-truth dot maps (density map generation), e.g., by convolving with a Gaussian kernel. Second, deep learning models are designed to predict a density map from an input image (density map estimation). Most research efforts have concentrated on the density map estimation problem, while the problem of density map generation has not been adequately explored. In particular, the density map could be considered as an intermediate representation used to train a crowd counting network. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used. To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. Then, we propose an adaptive density map generator, which takes the annotation dot map as input, and learns a density map representation for a counter. The counter and generator are trained jointly within an end-to-end framework. The experiment results on popular counting datasets confirm the effectiveness of the proposed learnable density map representations.},
author = {Wan, Jia and Chan, Antoni},
doi = {10.1109/ICCV.2019.00122},
file = {:Users/janerik/Library/Application Support/Mendeley Desktop/Downloaded/Wan, Chan - 2019 - Adaptive density map generation for crowd counting.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Crowd Counting},
pages = {1130--1139},
title = {{Adaptive density map generation for crowd counting}},
volume = {2019-Octob},
year = {2019}
}

@InProceedings{Shi_2018_CVPR,
author = {Shi, Zenglin and Zhang, Le and Liu, Yun and Cao, Xiaofeng and Ye, Yangdong and Cheng, Ming-Ming and Zheng, Guoyan},
title = {Crowd Counting With Deep Negative Correlation Learning},
booktitle = {CVPR},
year = {2018}
}

@inproceedings{shi2019counting,
  title={Counting with focus for free},
  author={Shi, Zenglin and Mettes, Pascal and Snoek, Cees GM},
  booktitle={ICCV},
  pages={4200--4209},
  year={2019}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% FLOW ESTIMATION %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Memin1998,
abstract = {In this paper, we address the issue of recovering and segmenting the apparent velocity field in sequences of images. As for motion estimation, we minimize an objective function involving two robust terms. The first one cautiously captures the optical flow constraint, while the second (a priori) term incorporates a discontinuity-preserving smoothness constraint. To cope with the nonconvex minimization problem thus defined, we design an efficient deterministic multigrid procedure. It converges fast toward estimates of good quality, while revealing the large discontinuity structures of flow fields. We then propose an extension of the model by attaching to it a flexible object-based segmentation device based on deformable closed curves (different families of curve equipped with different kinds of prior can be easily supported). Experimental results on synthetic and natural sequences are presented, including an analysis of sensitivity to parameter tuning. {\textcopyright} 1998 IEEE.},
author = {M{\'{e}}min, Etienne and P{\'{e}}rez, Patrick},
doi = {10.1109/83.668027},
file = {:Users/janerik/Downloads/1998{\_}ieeeip{\_}memin2.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Closed segmenting curve,Incremental multiresolution,Motion segmentation,Multigrid nonconvex minimization,Optical flow,Robust estimators},
mendeley-groups = {Flow Estimation},
number = {5},
pages = {703--719},
title = {{Dense estimation and object-based segmentation of the optical flow with robust techniques}},
volume = {7},
year = {1998}
}
@article{Wedel2009,
abstract = {The accurate estimation of motion in image sequences is of central importance to numerous computer vision applications. Most competitive algorithms compute flow fields by minimizing an energy made of a data and a regularity term. To date, the best performing methods rely on rather simple purely geometric regularizes favoring smooth motion. In this paper, we revisit regularization and show that appropriate adaptive regularization substantially improves the accuracy of estimated motion fields. In particular, we systematically evaluate regularizes which adaptively favor rigid body motion (if supported by the image data) and motion field discontinuities that coincide with discontinuities of the image structure. The proposed algorithm relies on sequential convex optimization, is real-time capable and outperforms all previously published algorithms by more than one average rank on the Middlebury optic flow benchmark. {\textcopyright}2009 IEEE.},
author = {Wedel, Andreas and Cremers, Daniel and Pock, Thomas and Bischof, Horst},
doi = {10.1109/ICCV.2009.5459375},
file = {:Users/janerik/Downloads/10.1.1.364.4370.pdf:pdf},
isbn = {9781424444205},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Flow Estimation},
pages = {1663--1668},
title = {{Structure- and motion-adaptive regularization for high accuracy optic flow}},
year = {2009}
}

@article{Bruhn2005,
abstract = {Differential methods belong to the most widely used techniques for optic flow computation in image sequences. They can be classified into local methods such as the Lucas-Kanade technique or Big{\"{u}}n's structure tensor method, and into global methods such as the Horn/Schunck approach and its extensions. Often local methods are more robust under noise, while global techniques yield dense flow fields. The goal of this paper is to contribute to a better understanding and the design of novel differential methods in four ways: (i) We juxtapose the role of smoothing/regularisation processes that are required in local and global differential methods for optic flow computation, (ii) This discussion motivates us to describe and evaluate a novel method that combines important advantages of local and global approaches: It yields dense flow fields that are robust against noise, (iii) Spatiotemporal and nonlinear extensions as well as multiresolution frameworks are presented for this hybrid method, (iv) We propose a simple confidence measure for optic flow methods that minimise energy functionals. It allows to sparsify a dense flow field gradually, depending on the reliability required for the resulting flow. Comparisons with experiments from the literature demonstrate the favourable performance of the proposed methods and the confidence measure.},
author = {Bruhn, Andr{\'{e}}s and Weickert, Joachim and Schn{\"{o}}rr, Christoph},
doi = {10.1023/B:VISI.0000045324.43199.43},
file = {:Users/janerik/Downloads/bruhn-ijcv05c.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Confidence measures,Differential techniques,Optic flow,Partial differential equations,Performance evaluation,Structure tensor,Variational methods},
mendeley-groups = {Flow Estimation},
number = {3},
pages = {1--21},
title = {{Lucas/Kanade meets Horn/Schunck: Combining local and global optic flow methods}},
volume = {61},
year = {2005}
}


@article{Brox2014,
abstract = {We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
author = {Brox, Thomas and Papenberg, Nils and Weickert, Joachim},
file = {:Users/janerik/Downloads/brox-eccv04-of-2.pdf:pdf},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Flow Estimation},
number = {May},
pages = {25--36},
title = {{High Accuracy Optical Flow Estimation based on warping - presentation}},
volume = {3024},
year = {2014}
}

@article{Pock2008,
author = {Pock, Thomas and Schoenemann, Thomas and Graber, Gottfried and Bischof, Horst},
doi = {10.1007/978-3-540-88690-7},
file = {:Users/janerik/Downloads/Learning{\_}Optical{\_}Flow.pdf:pdf},
isbn = {978-3-540-88689-1},
issn = {0302-9743},
mendeley-groups = {Flow Estimation},
number = {May 2014},
title = {{Learning optical flow}},
url = {http://link.springer.com/10.1007/978-3-540-88690-7},
volume = {5304},
year = {2008}
}

@article{Ranjan2017,
abstract = {We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96{\%} smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small ({\textless} 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning.},
archivePrefix = {arXiv},
arxivId = {1611.00850},
author = {Ranjan, Anurag and Black, Michael J.},
doi = {10.1109/CVPR.2017.291},
eprint = {1611.00850},
file = {:Users/janerik/Downloads/1611.00850.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
mendeley-groups = {Flow Estimation},
pages = {2720--2729},
title = {{Optical flow estimation using a spatial pyramid network}},
volume = {2017-January},
year = {2017}
}
@article{Hui2018,
abstract = {FlowNet2 [14], the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at github.com/twhui/LiteFlowNet.},
archivePrefix = {arXiv},
arxivId = {1805.07036},
author = {Hui, Tak Wai and Tang, Xiaoou and Loy, Chen Change},
doi = {10.1109/CVPR.2018.00936},
eprint = {1805.07036},
file = {:Users/janerik/Downloads/1805.07036.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Flow Estimation},
pages = {8981--8989},
title = {{LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation}},
year = {2018}
}




%%%%%%%%%%%%
%%%% UNSUPERVISED FLOW %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Yu2016,
abstract = {Recently, convolutional networks (convnets) have proven useful for predicting optical flow. Much of this success is predicated on the availability of large datasets that require expensive and involved data acquisition and laborious labelling. To bypass these challenges, we propose an unsupervised approach (i.e., without leveraging groundtruth flow) to train a convnet end-to-end for predicting optical flow between two images. We use a loss function that combines a data term that measures photometric constancy over time with a spatial term that models the expected variation of flow across the image. Together these losses form a proxy measure for losses based on the groundtruth flow. Empirically, we show that a strong convnet baseline trained with the proposed unsupervised approach outperforms the same network trained with supervision on the KITTI dataset.},
archivePrefix = {arXiv},
arxivId = {1608.05842},
author = {Yu, Jason J. and Harley, Adam W. and Derpanis, Konstantinos G.},
doi = {10.1007/978-3-319-49409-8_1},
eprint = {1608.05842},
file = {:Users/janerik/Downloads/1608.05842.pdf:pdf},
isbn = {9783319494081},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {To Read,Flow Estimation},
pages = {3--10},
title = {{Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness}},
volume = {9915 LNCS},
year = {2016}
}


@article{Liu2008,
abstract = {Obtaining ground-truth motion for arbitrary, real-world video sequences is a challenging but important task for both algorithm evaluation and model design. Existing ground-truth databases are either synthetic, such as the Yosemite sequence, or limited to indoor, experimental setups, such as the database developed in [5]. We propose a human-in-loop methodology to create a ground-truth motion database for the videos taken with ordinary cameras in both indoor and outdoor scenes, using the fact that human beings are experts at segmenting objects and inspecting the match between two frames. We designed an interactive computer vision system to allow a user to efficiently annotate motion. Our methodology is cross-validated by showing that human annotated motion is repeatable, consistent across annotators, and close to the ground truth obtained by [5]. Using our system, we collected and annotated 10 indoor and outdoor real-world videos to form a ground-truth motion database. The source code, annotation tool and database is online for public evaluation and benchmarking. {\textcopyright}2008 IEEE.},
author = {Liu, Ce and Freeman, William T. and Adelson, Edward H. and Weiss, Yair},
doi = {10.1109/CVPR.2008.4587845},
file = {:Users/janerik/Downloads/motion.pdf:pdf},
isbn = {9781424422432},
journal = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
mendeley-groups = {Flow Estimation},
title = {{Human-assisted motion annotation}},
year = {2008}
}


@article{Yu2016,
abstract = {Recently, convolutional networks (convnets) have proven useful for predicting optical flow. Much of this success is predicated on the availability of large datasets that require expensive and involved data acquisition and laborious labelling. To bypass these challenges, we propose an unsupervised approach (i.e., without leveraging groundtruth flow) to train a convnet end-to-end for predicting optical flow between two images. We use a loss function that combines a data term that measures photometric constancy over time with a spatial term that models the expected variation of flow across the image. Together these losses form a proxy measure for losses based on the groundtruth flow. Empirically, we show that a strong convnet baseline trained with the proposed unsupervised approach outperforms the same network trained with supervision on the KITTI dataset.},
archivePrefix = {arXiv},
arxivId = {1608.05842},
author = {Yu, Jason J. and Harley, Adam W. and Derpanis, Konstantinos G.},
doi = {10.1007/978-3-319-49409-8_1},
eprint = {1608.05842},
file = {:Users/janerik/Downloads/1608.05842.pdf:pdf},
isbn = {9783319494081},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {To Read,Flow Estimation},
pages = {3--10},
title = {{Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness}},
volume = {9915 LNCS},
year = {2016}
}
@article{Janai2018,
abstract = {Learning optical flow with neural networks is hampered by the need for obtaining training data with associated ground truth. Unsupervised learning is a promising direction, yet the performance of current unsupervised methods is still limited. In particular, the lack of proper occlusion handling in commonly used data terms constitutes a major source of error. While most optical flow methods process pairs of consecutive frames, more advanced occlusion reasoning can be realized when considering multiple frames. In this paper, we propose a framework for unsupervised learning of optical flow and occlusions over multiple frames. More specifically, we exploit the minimal configuration of three frames to strengthen the photometric loss and explicitly reason about occlusions. We demonstrate that our multi-frame, occlusion-sensitive formulation outperforms existing unsupervised two-frame methods and even produces results on par with some fully supervised methods.},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Ranjan, Anurag and Black, Michael and Geiger, Andreas},
doi = {10.1007/978-3-030-01270-0_42},
file = {:Users/janerik/Downloads/Janai2018ECCV.pdf:pdf},
isbn = {9783030012694},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Flow Estimation},
pages = {713--731},
title = {{Unsupervised Learning of Multi-Frame Optical Flow with Occlusions}},
volume = {11220 LNCS},
year = {2018}
}

@article{Meister2018,
abstract = {In the era of end-to-end deep learning, many advances in computer vision are driven by large amounts of labeled data. In the optical flow setting, however, obtaining dense per-pixel ground truth for real scenes is difficult and thus such data is rare. Therefore, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios continues to be a challenge. Inspired by classical energy-based optical flow methods, we design an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to circumvent the need for ground truth flow. On the KITTI benchmarks, our unsupervised approach outperforms previous unsupervised deep networks by a large margin, and is even more accurate than similar supervised methods trained on synthetic datasets alone. By optionally fine-tuning on the KITTI training data, our method achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.},
archivePrefix = {arXiv},
arxivId = {1711.07837},
author = {Meister, Simon and Hur, Junhwa and Roth, Stefan},
eprint = {1711.07837},
file = {:Users/janerik/Library/Application Support/Mendeley Desktop/Downloaded/Meister, Hur, Roth - 2018 - UnFlow Unsupervised learning of optical flow with a bidirectional census loss.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
mendeley-groups = {Flow Estimation},
pages = {7251--7259},
title = {{UnFlow: Unsupervised learning of optical flow with a bidirectional census loss}},
year = {2018}
}


@article{Sreenu2019,
abstract = {Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agriculture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big data. CCTV cameras are implemented in all places where security having much importance. Manual surveillance seems tedious and time consuming. Security can be defined in different terms in different contexts like theft identification, violence detection, chances of explosion etc. In crowded public places the term security covers almost all type of abnormal events. Among them violence detection is difficult to handle since it involves group activity. The anomalous or abnormal activity analysis in a crowd video scene is very difficult due to several real world constraints. The paper includes a deep rooted survey which starts from object recognition, action recognition, crowd analysis and finally violence detection in a crowd environment. Majority of the papers reviewed in this survey are based on deep learning technique. Various deep learning methods are compared in terms of their algorithms and models. The main focus of this survey is application of deep learning techniques in detecting the exact count, involved persons and the happened activity in a large crowd at all climate conditions. Paper discusses the underlying deep learning implementation technology involved in various crowd video analysis methods. Real time processing, an important issue which is yet to be explored more in this field is also considered. Not many methods are there in handling all these issues simultaneously. The issues recognized in existing methods are identified and summarized. Also future direction is given to reduce the obstacles identified. The survey provides a bibliographic summary of papers from ScienceDirect, IEEE Xplore and ACM digital library.},
author = {Sreenu, G. and {Saleem Durai}, M. A.},
doi = {10.1186/s40537-019-0212-5},
file = {:Users/janerik/Downloads/out.pdf:pdf},
isbn = {4053701902},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big data,Crowd analysis,Deep learning,Video surveillance},
number = {1},
pages = {1--28},
publisher = {Springer International Publishing},
title = {{Intelligent video surveillance: a review through deep learning techniques for crowd analysis}},
url = {https://doi.org/10.1186/s40537-019-0212-5},
volume = {6},
year = {2019}
}


@article{Chan2012,
abstract = {An approach to the problem of estimating the size of inhomogeneous crowds, which are composed of pedestrians that travel in different directions, without using explicit object segmentation or tracking is proposed. Instead, the crowd is segmented into components of homogeneous motion, using the mixture of dynamic-texture motion model. A set of holistic low-level features is extracted from each segmented region, and a function that maps features into estimates of the number of people per segment is learned with Bayesian regression. Two Bayesian regression models are examined. The first is a combination of Gaussian process regression with a compound kernel, which accounts for both the global and local trends of the count mapping but is limited by the real-valued outputs that do not match the discrete counts. We address this limitation with a second model, which is based on a Bayesian treatment of Poisson regression that introduces a prior distribution on the linear weights of the model. Since exact inference is analytically intractable, a closed-form approximation is derived that is computationally efficient and kernelizable, enabling the representation of nonlinear functions. An approximate marginal likelihood is also derived for kernel hyperparameter learning. The two regression-based crowd counting methods are evaluated on a large pedestrian data set, containing very distinct camera views, pedestrian traffic, and outliers, such as bikes or skateboarders. Experimental results show that regression-based counts are accurate regardless of the crowd size, outperforming the count estimates produced by state-of-the-art pedestrian detectors. Results on 2 h of video demonstrate the efficiency and robustness of the regression-based crowd size estimation over long periods of time. {\textcopyright} 2011 IEEE.},
author = {Chan, Antoni B. and Vasconcelos, Nuno},
doi = {10.1109/TIP.2011.2172800},
file = {:Users/janerik/Downloads/tip12-pedcount.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bayesian regression,Gaussian processes,Poisson regression,crowd analysis,surveillance},
mendeley-groups = {Datasets},
number = {4},
pages = {2160--2177},
pmid = {22020684},
title = {{Counting people with low-level features and bayesian regression}},
volume = {21},
year = {2012}
}


@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/janerik/Downloads/1412.6980.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}

