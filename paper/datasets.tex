\chapter{Datasets}
In this chapter we explain the datasets in more depth. First we explain the requirements of the dataset to correctly train and evaluate each dataset. To compensate for lack of some required labelling a tool is written and explained. Lastly all the datasets are explained in more depth.

\section{Requirements}
For training and evaluation we need two different methods of labelling. For the density map generation the position of each pedestrian is required for the frames which are used for training. For evaluation the line crossing it is required to label the amount of pedestrians crossing the LOI from each side. Ideally the training set is purely labeled with head-tags and the evaluation set only with line crossing labelling.

\section{Labelling}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.6\textwidth]{images/labeller}
\caption{User interface of the labeller}
\label{fig:labeller}
\end{figure}

Several Crowd Counting datasets provide sequences of frames with corresponding pedestrian labelling. However most of those datasets don't provide line crossing labelling. Therefore I build a tool to label videos for line crossing (Figure \ref{fig:labeller}).
The labeller loads a video from the unlabeled videos. In this video the user can label multiple lines by first clicking on the video to draw a line and afterwards fill in the amount of pedestrians crossing the line during the video. Additionally the user can scroll and view through the video using multiple manipulations.


\section{Datasets}
\begin{figure}[h]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{images/dataset_ucsd}
  \caption{UCSD}
  \label{fig:dataset_ucsd}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{images/dataset_fudan}
  \caption{Fudan-ShanghaiTech}
  \label{fig:dataset_fudan}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{images/dataset_aicity}
  \caption{AI City Challenge}
  \label{fig:dataset_aicity}
\end{subfigure}%
\caption{Samples of each dataset}
\label{fig:datasets}
\end{figure}

\subsection{UCSD}
The UCSD Pedestrian dataset \cite{Chan2008} is a public dataset created in 2008. The dataset contains a video of a path where people cross each other (See figure \ref{fig:dataset_ucsd}). The dataset is 720x480 in resolution. The original videos are in 30fps, but are downscaled to 10fps. A total of 5680 frames are captured. The first 2000 frames contain line of interest labelling \cite{Ma2013}. Frames 0-599 and 1400-1999 are used for testing. The rest of the frames are used as training samples.

For the training samples all the locations of each pedestrian is provided, so no extra labelling required for density generation \cite{Chan2012}.

\subsection{Fudan-ShanghaiTech}
The Fudan ShanghaiTech dataset \cite{Fang2019} is a public dataset with 100 videos of 13 different scenes. Each video contains 6 seconds of footage at 25 fps and have a resolution of 1920x1080 or 1280x720 (Sample of scene in figure \ref{fig:dataset_fudan}). The scenes have between 20-100 pedestrians per frame. In each frame the pedestrians in the frame are labeled with a bounding-box and a center-point of the bounding-box. The dataset contains 60 training videos and 40 testing videos.

The lack of trajectories and custom line crossing labelling requires the use of the custom build labeller (Figure \ref{fig:labeller}). This is done on the 40 videos of the test set.

\subsection{AI City Challenge}
The AI City Challenge dataset is a huge dataset of cars crossing on a road. The dataset contains 20 different scenes, whereas 11 of the scenes are crossroads. A total of around 2.5 hour footage is captured at 10FPS (1 hour for training, 1.5 hours for testing). Each vehicle which is labeled as a bounding box, which can be used for the centre-point. Additionally a set of Line of Interest labelling is provided. So no extra labelling for this dataset is required.

\todo{Reducing the amount of footage to create a doable benchmark, then define this here as well}