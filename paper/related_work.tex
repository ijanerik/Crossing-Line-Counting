\chapter{Related Work}
In this chapter we try to explain a couple of key papers on which the proposed methods are build.

\section{Region of Interest}
\subsection{CSRNet}
In 2018 CSRNet \cite{li2018csrnet} was introduced. This model had a massive improvement over earlier proposed models in the Region of Interest field. In the proposed model they proposed the use of dilated convolutions which use convolution filters over a much wider area. With this method the area a convolution filter stretches is much higher, without increasing the amount of processing time.
\todo{Explain in depth the contribution}
\todo{We don't use CSRNet, so maybe just skip this part}
\section{Flow Estimation}

\subsection{PWCNet}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/pwcnet_improvements}
\caption{PWCNet compared to other existing flow estimation architectures}
\label{fig:pwc_compare}
\end{figure}

A popular Flow Estimation network is PWCNet \cite{sun_pwc-net_2018}. It uses the original ideas of FlowNet, but it improves FlowNet in a lot of ways. FlowNet traditionally is used to fully predict the flow with a neural network. PWCNet massively reduces the number of weights (See figure \ref{fig:pwc_compare}), which results in faster training and much quicker prediction. Additionally the network shows a higher accuracy on several benchmarks.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/pwcnet_approach}
\caption{PWCNet architecture}
\label{fig:pwc_approach}
\end{figure}

PWCNet uses a pyramid shape architecture to predict the velocity map (See figure \ref{fig:pwc_approach}). The encoder encodes both input frames seperately into two feature volumes. During decoding several decoding steps are taken. After the initial velocity map estimation, each decoding step upscales the so-far estimated map and further refines the map. At the start of each decoding step the estimated velocity map is used to warp the feature volume of the second frame and correlate this warped volume with the volume of the first image to focus on area of refinement in the velocity map.

\subsection{DDFlow}
In DDFlow \cite{liu_ddflow_2019} a general method is proposed to estimate a velocity map in an unsupervised matter. Earlier methods already proposed several methods to optimize using a photometric loss and ignore occluded-pixels during loss calculation. However all these earlier methods used handcrafted methods to estimate the occluded-pixels.

The paper proposes a method which learns occluded-pixels using a distillation from unlabeled data without supervision of humans to label the ground truth. Because of the generality of the method, the method can be wrapped around all existing supervised flow estimation architectures.

The method uses a teacher and a student approach to train the occluded pixels. The teacher network is trained on all the non-occluded pixels, with exclusion of the occluded pixels using only the photometric loss with occlusion-awareness. After training the teacher network, the velocity maps of the teacher network are used as ground-truth for the student network.

The student network is then trained on patches of the original predicted velocity map. On the borders of the patches, occluded pixels will appear, because moving pixels from inside the patched frame will move to outside the patched frame. These border pixels will be marked as occluded pixel by the student network, but will be non-occluded pixels according to the ground-truth of the teacher network.

\todo{Should this part be moved to background or keep in related work? Seems both valid.}
\begin{equation}
\begin{aligned}
L_{p}=& \sum \psi\left(I_{1}-I_{2}^{w}\right) \odot\left(1-O_{f}\right) / \sum\left(1-O_{f}\right) \\
&+\sum \psi\left(I_{2}-I_{1}^{w}\right) \odot\left(1-O_{b}\right) / \sum\left(1-O_{b}\right)
\end{aligned}
\label{eq:photometric_loss}
\end{equation}

The photometric loss with occlusion-awareness proposed in the earlier papers is defined in equation \ref{eq:photometric_loss}. Where $I_1$ and $I_2$ are the full input frames and $I_i^w$ the backwarped image based on the predicted velocity map. Additionally $O_b$ and $O_f$ are masks for the occluded pixels. These maps are calculated by checking if the mismatch between forward flow and backward flow is too large.

For the student model the photometric loss is used together with the loss for occlusion (equation \ref{eq:student_pm_loss}). The loss of the student model then just $L_p+L_o$. In equation \ref{eq:student_pm_loss}, $\widetilde{\mathrm{w}}_{f}$ and $\widetilde{\mathrm{w}}_{b}$ are the predicted velocity map forward and backward using the student model. $\mathrm{w}_{b}^{p}$ and $\mathrm{w}_{f}^{p}$ are the cropped patches from the ground-truth velocity map predicted by the teacher model. $M_f$ and $M_b$ are just defined as the difference of occluded pixels between the ground-truth of the parent and the teacher ($M_{f}=\operatorname{clip}\left(\widetilde{O}_{f}-O_{f}^{p}, 0,1\right)$. These pixels are not occluded in the full frame and occluded in the patch. This means that the ground-truth using distillation is available.

\begin{equation}
\begin{aligned}
L_{o}=&\sum \psi\left(\mathrm{w}_{f}^{p}-\widetilde{\mathrm{w}}_{f}\right) \odot M_{f} / \sum M_{f} \\
&+\sum \psi\left(\mathrm{w}_{b}^{p}-\widetilde{\mathrm{w}}_{b}\right) \odot M_{b} / \sum M_{b}
\end{aligned}
\label{eq:student_pm_loss}
\end{equation}

\section{Line of Interest}

\subsection{Instant LOI counting}
Zhao et al. \cite{leibe_crossing-line_2016} introduce a new approach to calculate the amount of pedestrians crossing the Line of Interest. Instead of slicing a range of consecutive frames around the Line of Interest and stitching them together, they present a method that directly predicts the amount crossing pedestrians using two consecutive frames.

They use both a density map and velocity map and finally merge them together to obtain the LOI counts. Both the maps are trained fully supervised. Because annotating raw velocity maps is a very hard task, they simplify the velocity map in disk-shaped area's around the pedestrian locations. Annotating each frame is still very demanding, but lowers the amount of labeling significantly in comparing to full velocity maps.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{images/zhao16_model}
\caption{Pipeline of Zhao et al. with FlownetSimple as architecture, where in the last layer both the density map as the velocity map are predicted}
\label{fig:zhao_model}
\end{figure}


They are as well the first who predict both the density map and the velocity map in a single Convolutional Neural Network. A network they use a  simplified model of FlowNet. Because the velocity map and density map are so similar in their location, they predict both the velocity map and the density map in the final layer (See figure \ref{fig:zhao_model}).

To finally merge both the density map and velocity map together they use a pixel-wise approach. Where they turn te density map and velocity map in a directional counting map $C_t=D_t \otimes V_t$. According to the paper the directional counting map gives the amount of pedestrians crossing that pixel between the time of the two frames.
\begin{equation}
\begin{aligned}
c_{1, t} =& \sum_{\left\{p \mid \cos \left(\theta_{p}\right) \geq 0\right\}} \sqrt{C_{t, x}(p)^{2}+C_{t, y}(p)^{2}} \cdot \cos \left(\theta_{p}\right) \\
c_{2, t} =& \sum_{\left\{p \mid \cos \left(\theta_{p}\right)<0\right\}} \sqrt{C_{t, x}(p)^{2}+C_{t, y}(p)^{2}} \cdot\left(-\cos \left(\theta_{p}\right)\right),
\end{aligned}
\label{eq:zhao_sum_norm}
\end{equation}
\begin{equation}
c_{1}=\sum_{\{t \mid t \in T\}} c_{1, t}, \quad c_{2}=\sum_{\{t \mid t \in T\}} c_{2, t}
\label{eq:zhao_timeframe_sum}
\end{equation}

To calculate per frame pair the amount of pedestrians crossed the line, a set of locations $p$ around the LOI is taken. Then the directional counting map is normalized and summed together in equation \ref{eq:zhao_sum_norm}, where $\theta_p$ is the angle between $V_t(p)$ and the LOI.

To calculate the total line crosses over a certain timeframe, the results of equation \ref{eq:zhao_sum_norm} can be summed as in equation \ref{eq:zhao_timeframe_sum}.


\subsection{Region-level LOI counting}
Zheng et al. \cite{zheng_cross-line_2019} provides a fast method of predicting the Line of Interest. The paper outperforms \cite{leibe_crossing-line_2016} in the UCSD benchmark. Zheng et al. discusses the problems with Neural Networks and the complexity the models, which makes it hard to run the models real time. It therefore introduces a non-CNN based method based on a SVM, linear regression and the Lucas-Kanade optical flow tracker.

It uses the idea of \cite{leibe_crossing-line_2016} to discard the slicing method and uses pair-wise prediction and uses the same method to merge density and velocity. Because the methods used in \cite{zheng_cross-line_2019} are not on a pixel-level, a method is proposed to bin on a region-level. In equation \ref{eq:zheng_region_vel} a single velocity $v_{t,r}$ is calculated with a weighted average over all moving keypoints inside the region.

\begin{equation}
	v_{t,r} = \frac{\sum_p w_r(p) \cdot v^{(t)}_{towards}(p)}{\sum_p w_r(p)}
	\label{eq:zheng_region_vel}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=0.33\textwidth]{images/zheng19_regions}
\caption{Regions around the LOI with skewness and normalized to a straight LOI}
\label{fig:zheng_skew}
\end{figure}

Additional to the region-level LOI counting, they introduce the method to skew the region to take into account the perspective of the camera view (See figure \ref{fig:zheng_skew}). This helps the SVM and linear regressor to more accurately predict the amount of pedestrians inside the region.
