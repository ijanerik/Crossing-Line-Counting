\chapter{Implementation}
In this chapter details about the actual implementation are explained in more depth.

\section{Models}
\subsection{Baseline 1}
As baseline the model of \cite{Zhang2016} is used. Because of the difference in task, parts of the proposed method can't used. Only the proposed model therefore is used as baseline, where the exact network is shown in figure \ref{fig:zhao_model}.

Due to full shared nature of the network the assumption is that the network will perform very poor when predicting both the density map and the unsupervised flow map. Therefore we propose a second baseline as well.

\subsection{Baseline 2}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/implementation_baseline2}
\caption{CSRNet+PWCNet baseline}
\label{fig:unified_model}
\end{figure}

This stronger baseline is a combination of two independent models. CSRNet \cite{li2018csrnet} for predicting the density map and PWCNet \cite{sun_pwc-net_2018} for flow estimation. Both models perform very well in comparing to papers in their respective field. Therefore will this baseline be a very powerful baseline to compare to. The models are trained independently of each other with no shared loss function to further optimize their performance.

\subsection{Shared encoder}
The first proposed model shares the decoder of the PWCNet model (Full network in appendix, figure \ref{fig:unified_proposed_model}). The PWCNet makes use of a pyramide shaped architecture and the encoder provides the decoder with 6 feature maps ranging from 1/2 the size to 1/64 the size of the original image.

The decoder contains two essential processing blocks. The dilation block, which is a block with 4 conv-layers with a dilation of 2. Additionally an upscaling block is used, which first upscales the input by two and then refines by 2 conv-layers. All conv-layers use a kernel of 3x3 and a stride of 1.

Each of the four tiniest feature map are processed by a dilation block. The smallest feature map is processed first and individually upscaled using the upscaling block. The second and third feature maps are first concatenated with the earlier feature maps and then upscaled.

After processing the fourth feature map and concatenation the upscaled feature maps, the features are processed by two dilation blocks which then predicts the density map using a single output layer.




\subsection{Flow enhancing}
The second model proposed enhances the feature maps with the output of the flow map. The flow enhanced model uses the first proposed model as base decoder. Instead of only decoding the feature maps on each level, the output flow map is reshaped and concatenated to each feature map. Which results the information of the flow on each level available.


\section{Environment}

\subsection{Maxing filter}
During experiments the maxing filter is optimized per dataset. For the Fudan-ShanghaiTech dataset two times a maxing filter is applied width a distance of 6px. For the UCSD dataset two times a maxing filter of 4px is applied.

The maxing filter is applied multiple times to avoid a huge maxing filter which increases the processing time exponentially after a certain distance.

\subsection{Line Crossing}
In the equation \ref{eq:pixel_cross} all the parameters for the Line Crossed are shown. The width of the line (parameter $d$) is the last parameter which is not defined. In preliminary results the difference is width is minimal, during all the experiments a width of 20px is used therefore.

\subsection{Loss function}
\begin{equation}
\label{eq:loss_total}
	L_{t} = L_{v} + \lambda \cdot L_{c}
\end{equation}

The loss function for the final training is given in equation \ref{eq:loss_total}. Where $L_{v}$ is the loss function for the velocity map. We finally applied the occlussion photometric loss on all results (Equation \ref{eq:photometric_loss}).

$L_{c}$ is used for the density map. Where the L2 loss (Mean squared error) is applied for comparing the ground truth density map with the predicted map.

For $\lambda$ a default of 5 is applied when not mentioned else.


\subsection{Optimizer}
For all the experiments the Adam optimizer is used with a learning-rate of $5e^{-5}$. No regularization is applied.

\subsection{Augmentation}
To augment the dataset several augmentations will be applied on the trainingssamples. First a crop of the image is made. Ranging from $\nicefrac{1}{3}$ and $\nicefrac{1}{6}$ of the total image size. Then the cropped image is resized to a size of $\nicefrac{1}{4}$ of the original image (So $\nicefrac{1}{2}$ the width and $\nicefrac{1}{2}$ the height). Lastly the cropped image is half of the time flipped horizontally.

\subsection{Metrics}
For both the ROI and the LOI the Mean Average Error and the Mean Squared Error are used. Additionally the LOI uses the Relative Mean Average Error.

\begin{equation}
\label{eq:mae_roi}
	MAE = \frac{1}{n}\sum^n_{i=1}|C_i-P_c^{(i)}|
\end{equation}

\begin{equation}
\label{eq:mse_roi}
	MSE = \frac{1}{n}\sum^n_{i=1}(C_i-P_c^{(i)})^2
\end{equation}

For the ROI the MAE and the MSE are defined as equation \ref{eq:mae_roi} and \ref{eq:mse_roi}. Where $P_c^{(i)}$ is the predicted density map for the given frame.

\begin{equation}
\label{eq:mae_loi}
	MAE = \frac{1}{n}\sum^n_{i=1}|G_l^{(i)}-P_l^{(i)}|+|G_r^{(i)}-P_r^{(i)}|
\end{equation}

\begin{equation}
\label{eq:mse_loi}
	MSE = \frac{1}{n}\sum^n_{i=1}(G_l^{(i)}-P_l^{(i)})^2+(G_r^{(i)}-P_r^{(i)})^2
\end{equation}

\begin{equation}
\label{eq:rmae_loi}
	RMAE = \frac{1}{n}\sum^n_{i=1}\frac{|G_l^{(i)}-P_l^{(i)}|+|G_r^{(i)}-P_r^{(i)}|}{G_l^{(i)} + G_r^{(i)}}
\end{equation}

For the LOI the MAE and MSE are defined as quation \ref{eq:mae_loi} and \ref{eq:mse_loi}. The RMAE is simply defined as in equation \ref{eq:rmae_loi}. Where $G_l^{(i)}$ is the ground truth for sample $i$ for side left-to-right. And $P_r^{(i)}$ is the predicted value for right-to-left.
